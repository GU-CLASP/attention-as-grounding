# attention-as-grounding

# Citation

## Citation

  > [Nikolai Ilinykh and Simon Dobnik. 2022. Attention as Grounding: Exploring Textual and Cross-Modal Attention on Entities and Relations in Language-and-Vision Transformer. In Findings of the Association for Computational Linguistics: ACL 2022, pages 4062â€“4073, Dublin, Ireland. Association for Computational Linguistics.](https://aclanthology.org/2022.findings-acl.320/)

This paper can be also found [here](acl-2022/paper-1844.pdf).

If you find our models or results useful, please cite as follows:

```
@inproceedings{madureira-schlangen-2020-incremental,
    title = "Incremental Processing in the Age of Non-Incremental Encoders: An Empirical Assessment of Bidirectional Models for Incremental {NLU}",
    author = "Madureira, Brielen  and
      Schlangen, David",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.26",
    doi = "10.18653/v1/2020.emnlp-main.26",
    pages = "357--374",
}   
```
